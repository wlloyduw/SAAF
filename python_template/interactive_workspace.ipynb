{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaaSET Notebook\n",
    "\n",
    "This Jupyter Notebook provides an interactive platform for FaaS function development, testing, running experiments, and processing results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Setup Environment\n",
    "\n",
    "The FaaSET Notebook can be hosted using a variety of different environments. To simplify the setup process, the cells below can be used to automatically configure the environment on Google Colaboratory or Binder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Google Colaboratory\n",
    "!apt update && apt install git jq zip awscli parallel bc curl python3.8 -y\n",
    "!git clone https://www.github.com/wlloyduw/SAAF \n",
    "%pip install requests boto3 botocore tqdm numpy pandas matplotlib ipython jupyter kaleido plotly==5.3.1\n",
    "%pip install --upgrade awscli\n",
    "\n",
    "!wget https://bootstrap.pypa.io/get-pip.py\n",
    "!python3.8 get-pip.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.chdir(\"./SAAF/jupyter_workspace/src/\")\n",
    "os.mkdir(\"../../test/history/interactiveExperiment\")\n",
    "os.mkdir(\"/content/graphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Binder Environment\n",
    "%pip install --upgrade awscli\n",
    "!wget https://bootstrap.pypa.io/get-pip.py\n",
    "!python3.8 get-pip.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.mkdir(\"../../test/history/interactiveExperiment\")\n",
    "os.mkdir(\"/content/graphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS Credentials\n",
    "access_key = \"FILL THIS IN\"\n",
    "secret_key = \"FILL THIS IN\"\n",
    "region = \"us-east-1\"\n",
    "!printf $access_key\"\\n\"$secret_key\"\\n\"$region\"\\njson\\n\" | aws configure    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Notebook Setup\n",
    "\n",
    "Welcome to the FaaSET Jupyter notebook! This default notebook provides comments to guide you through all of the main features. If you run into errors or probls please make sure you have the AWS CLI properly configure so that you can deploy function with it, have Docker installed and running, gave execute permission to everything in the /jupyter_workspace and /test directory, and finally installed all the dependencies. You can use quickInstall.sh in the root folder to walk you through the setup process and install dependencies. Other environments may work but getting this notebook to work on cloud based platforms like Google Collab may be very difficult.\n",
    "\n",
    "Anyway, this first cell is just imports needed to setup the magic that goes on behind the scenes. Run it and it should return nothing. In this cell we define our config object, this object contains any information that we need to deploy functions, such as a role for functions on AWS Lambda. If all of your functions will use the same config object, you can set it globally by using setGlobalConfig. Any methods that take a config object will priorize the object passed to them over the global config.\n",
    "\n",
    "The setGlobalDeploy function defines that you want your cloud functions to be automatically deployed when they are ran. This can be disabled by setting the method to false.\n",
    "\n",
    "Function documentation available in jupyter_workspace/platforms/jupyter/interactive_helpers.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.realpath('..'))\n",
    "import FaaSET\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "Any function with the @cloud_function decorator will be uploaded to the cloud. Define platforms and memory settings in the decorator. \n",
    "Functions are tested locally and must run sucessfully before being deployed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Deploying Functions\n",
    "\n",
    "Here is your first cloud function! Creating cloud functions is as simple as writing python functions with (request, context) arguments and adding the @cloud_function decorator! See the two hello world functions below, they are nearly identical! But when we run them we will see that the CPU used on the cloud will be different than our local CPU returned by the SAAF inspector inspectCPUInfo method. That is because the function is running on AWS Lambda! \n",
    "\n",
    "You can add arguments to the cloud_function decorator to define the platform you would like to deploy to, the memory setting, and different context objects. Other arguments like references, requirements, and containerize can be used to change behavior.\n",
    "\n",
    "Cloud functions defined in this notebook do have a few limitations. The main one is that nothing outside the function is deployed to the cloud. That is why imports are inside the function, which is a little weird and can have an effect on what you can import. But for most things this is fine. \n",
    "\n",
    "Alongside deploying your function code, you can deploy files alongside this function by adding them to the src/includes_{function name} folder (This function will use src/includes_hello_world). This folder will be automatically created when the function is ran. You can include basically anything, files, scripts, python libraries, whatever you need.\n",
    "\n",
    "If everything is setup correct, all you need to do is run this code block and you'll get a hello_world function on AWS Lambda! If not all dependencies are installed you can use ./quickInstall.sh to download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@FaaSET.cloud_function(platform=\"aws\", config={\"role\": \"ENTER ROLE\"})\n",
    "def hello_world(request, context): \n",
    "    from SAAF import Inspector\n",
    "    inspector = Inspector()\n",
    "    inspector.inspectCPUInfo()\n",
    "    inspector.addAttribute(\"message\", \"Hello from the cloud \" + str(request[\"name\"]) + \"!\")\n",
    "    return inspector.finish()\n",
    "\n",
    "def hello_world_local(request, context): \n",
    "    from SAAF import Inspector\n",
    "    inspector = Inspector()\n",
    "    inspector.inspectCPUInfo()\n",
    "    inspector.addAttribute(\"message\", \"Hello from your computer \" + str(request[\"name\"]) + \"!\") \n",
    "    return inspector.finish()\n",
    "\n",
    "# Run our local hello_world function and check the CPU.\n",
    "local = hello_world_local({\"name\": \"Steve\"}, None)\n",
    "print(\"Local CPU: \" + local['cpuType'])\n",
    "\n",
    "# Run our cloud hello_world function and check the CPU.\n",
    "cloud = hello_world({\"name\": \"Steve\"}, None)\n",
    "print(\"Cloud CPU: \" + cloud['cpuType'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containers\n",
    "\n",
    "To create even more complex execution environments functions can be packaged as Docker containers and deployed to AWS Lambda and IBM Cloud Functions. The function below shows creating the Dockerfile and writing the same function as in the previous example.\n",
    "\n",
    "First we create the includes_ directory, this is a folder that anything contained in it will be deployed alongside your function. The FaaSET Notebook will look for Dockerfiles in this directory to use instead of the default Docker file. The default just contains the bare minimum to deploy a function as a Docker container. To create more complex environments it is recommended to create your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./includes_page_rank_container"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the includes directory we can write our Dockerfile. In this example we are using the Jupyter writefile magic so we can edit this file directly within the FaaSET Notebook. This Dockerfile includes everything needed to get the Debian python slim buster image running on AWS Lambda. The default Dockerfile will use Amazon Linux 2 rather than a Debian based image. Since we are creating this image from scratch, we need to install some dependencies and the AWS Lambda Runtime Interface Emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile includes_page_rank_container/Dockerfile\n",
    "FROM python:3.8-slim-buster\n",
    "RUN apt-get update\n",
    "RUN apt-get install -y wget\n",
    "ENV FUNCTION_DIR=\"/var/task\"\n",
    "RUN mkdir -p ${FUNCTION_DIR}\n",
    "COPY . ${FUNCTION_DIR}\n",
    "RUN pip install igraph\n",
    "RUN pip install \\\n",
    "        --target ${FUNCTION_DIR} \\\n",
    "        awslambdaric\n",
    "RUN curl -Lo /usr/local/bin/aws-lambda-rie https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie\n",
    "RUN chmod +x /usr/local/bin/aws-lambda-rie\n",
    "WORKDIR ${FUNCTION_DIR}\n",
    "COPY ./entry_script.sh /entry_script.sh\n",
    "RUN chmod +x /entry_script.sh\n",
    "ENTRYPOINT [ \"/entry_script.sh\" ]\n",
    "CMD [ \"lambda_function.lambda_handler\" ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our Dockerfile is created we can write the function in the exact same way as we did previously except using the containerize flag in the dectorator rather than defining requirements. Docker containers provide significant advantages as they allow dependencies to be installed in a much simpler way and significantly more configuration options that the default zip packaging method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cloud_function(platform=\"AWS Docker\", config={\"memory\": 1024})\n",
    "def page_rank_container(request, context):\n",
    "    from SAAF import Inspector \n",
    "    import igraph\n",
    "    \n",
    "    inspector = Inspector()\n",
    "    inspector.inspectAll()\n",
    "    \n",
    "    size = request.get('size')  \n",
    "    loops = request.get('loops')\n",
    "\n",
    "    for x in range(loops):\n",
    "        graph = igraph.Graph.Tree(size, 10)\n",
    "        result = graph.pagerank()  \n",
    "\n",
    "    inspector.inspectAllDeltas()\n",
    "    return inspector.finish()\n",
    "\n",
    "page_rank_container({\"size\": 10000, \"loops\": 5}, None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Experiments\n",
    "\n",
    "Use FaaS Runner to execute complex FaaS Experiments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: FaaS Runner Experiments\n",
    "\n",
    "Now, what's cooler than running a function on the cloud once? Running it multiple times! The run_experiment function allows you to create complex FaaS experiments. This function uses our FaaS Runner application to execute functions behind the scenes. It's primary purpose is to run multiple function requests across many threads. You define payloads in the payloads list, choose your memory setting (it will switch settings automatically) and define how many runs you want to do, across how many threads, and how many times you want to repeat the test with iterations. These are the most important parameters, but there are many more defined in the link below. \n",
    "\n",
    "After an experiment runs, the results are converted into a pandas dataframe that you can continue using in this notebook. For example you can use matplotlib to generate graphs (see below), or do any other form of data processing. \n",
    "\n",
    "Below are two different experiments for our functions. Execute them and generate graphs using the code cells below. You now have experienced all the functionality of the FaaSET Notebook! Happy FaaS developing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment parameters. For more detail see: https://github.com/wlloyduw/SAAF/tree/master/test\n",
    "hello_experiment = {\n",
    "  \"payloads\": [{\"name\": \"Bob\"}],\n",
    "  \"memorySettings\": [256, 512, 1024],\n",
    "  \"runs\": 20,\n",
    "  \"threads\": 5,\n",
    "  \"iterations\": 1\n",
    "}\n",
    "\n",
    "# Execute experiment\n",
    "hello_world_results = run_experiment(function=hello_world, experiment=hello_experiment)\n",
    "hello_world_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_rank_experiment = {\n",
    "  \"payloads\": [{\"size\": 50000, \"loops\": 5},\n",
    "                {\"size\": 100000, \"loops\": 5},\n",
    "                {\"size\": 150000, \"loops\": 5}],\n",
    "  \"memorySettings\": [512],\n",
    "  \"runs\": 60,\n",
    "  \"threads\": 30,\n",
    "  \"iterations\": 1,\n",
    "  \"shufflePayloads\": False\n",
    "}\n",
    "\n",
    "# Execute experiment\n",
    "page_rank_results = run_experiment(function=page_rank, experiment=page_rank_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions and Experiments can be written in the same cell!\n",
    "@cloud_function()\n",
    "def sleeper(request, context): \n",
    "    from SAAF import Inspector\n",
    "    import time\n",
    "    inspector = Inspector()\n",
    "    inspector.inspectAll()\n",
    "    time.sleep(request['time'])\n",
    "    inspector.inspectAllDeltas()\n",
    "    return inspector.finish()\n",
    "\n",
    "# Test function\n",
    "print(str(sleeper({\"time\": 1}, None)))\n",
    "\n",
    "# Define and execute experiment\n",
    "sleep_experiment = {\n",
    "  \"payloads\": [{\"time\": 5}],\n",
    "  \"memorySettings\": [2048, 4096, 6144],\n",
    "  \"runs\": 10,\n",
    "  \"threads\": 10,\n",
    "  \"iterations\": 1,\n",
    "}\n",
    "sleeper_results = run_experiment(function=sleeper, experiment=sleep_experiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Results\n",
    "\n",
    "FaaS Runner experiment results are parsed into a Pandas dataframe. This flexibility allows the ability to perform any kind of data processing that you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Graphing\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define figure style\n",
    "fig = make_subplots(specs = [[{\"secondary_y\": False}]])\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"center\", x=0.47),\n",
    "    margin=dict(t=0, b=1, l=1, r=1, autoexpand=True),\n",
    "    font=dict(size=16)\n",
    ")\n",
    "\n",
    "workloads = [sleeper_results, sleeper_results, sleeper_results]\n",
    "names = [\"2048 MB\", \"4096 MB\", \"6144 MB\"]\n",
    "targetMemory = [2048, 4096, 6144]\n",
    "\n",
    "finalData = pd.DataFrame()\n",
    "finalData['workloads'] = names\n",
    "\n",
    "cpuUsers = [] \n",
    "cpuIdles = []\n",
    "cpuKernels = []\n",
    "runtimes = []\n",
    "\n",
    "i = 0\n",
    "for workload in workloads:\n",
    "    cpuUsers.append(workload[workload['functionMemory'] == targetMemory[i]]['cpuUserDelta'].mean())\n",
    "    cpuIdles.append(workload[workload['functionMemory'] == targetMemory[i]]['cpuIdleDelta'].mean())\n",
    "    cpuKernels.append(workload[workload['functionMemory'] == targetMemory[i]]['cpuKernelDelta'].mean())\n",
    "    runtimes.append(workload[workload['functionMemory'] == targetMemory[i]]['runtime'].mean())\n",
    "    i += 1\n",
    "\n",
    "finalData['cpuUser'] = cpuUsers\n",
    "finalData['cpuIdle'] = cpuIdles\n",
    "finalData['cpuKernel'] = cpuKernels\n",
    "finalData['runtime'] = runtimes\n",
    "\n",
    "fig.add_trace(go.Bar(x = finalData[\"workloads\"],\n",
    "                y = finalData[\"cpuKernel\"], \n",
    "                name = \"CPU Kernel\", marker_color=\"rgba(179, 223, 146, 255)\"),\n",
    "                secondary_y=False)\n",
    "\n",
    "fig.add_trace(go.Bar(x = finalData[\"workloads\"],\n",
    "                y = finalData[\"cpuUser\"], \n",
    "                name = \"CPU User\", marker_color=\"rgba(0, 120, 179, 255)\"),\n",
    "                secondary_y=False)\n",
    "\n",
    "fig.add_trace(go.Bar(x = finalData[\"workloads\"],\n",
    "                y = finalData[\"cpuIdle\"], \n",
    "                name = \"CPU Idle\", marker_color=\"rgba(151, 209, 233, 255)\"),\n",
    "                secondary_y=False)\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Memory Setting\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"CPU Time (ms)\", secondary_y=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib and setup display.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Histogram of runtime\n",
    "plt.hist(page_rank_results['userRuntime'], 10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
